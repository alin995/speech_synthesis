# 语音合成学习（十三）学习笔记

---
在[深度学习](https://so.csdn.net/so/search?q=深度学习&spm=1001.2101.3001.7020)中有很多参数是我们在训练模型之前自己设定的，我们把这些参数就称为——**超参数**。其中主要超参数包括了:学习率、batch_size、[梯度下降](https://so.csdn.net/so/search?q=梯度下降&spm=1001.2101.3001.7020)、等。

## 常用参数整理

## 学习率 

  初始学习率一般设为：0.001~0.01之间。

选择一个好的学习率不仅可以加快模型的收敛，避免陷入局部最优，减少迭代的次数，同时可以提高模型的进度。下面我们就来说说 学习率。
  学习率是最重要的超参数，因为它以一种复杂的方式控制着模型的有效容量。如果在有限时间内，想提高模型性能，那么就调它。调其他的超参数需要监视训练和测试误差来判断模型是欠拟合还是过拟合。

学习率指的是每次参数更新时，参数沿着梯度方向更新的幅度大小。学习率越大，参数更新的幅度越大，模型收敛的速度也会更快；但是学习率过大，可能会导致模型不收敛或者收敛到不稳定的状态。相反，学习率越小，模型收敛的速度越慢，但是模型的稳定性会更好。

学习率通常高于前 100 次迭代后产生最佳性能的学习率。监视前几次迭代，选择一个比表现最好的学习率更高的学习率，同时注意避免不稳定的情况。如果学习率太高，我们的损失函数将开始在某点来回震荡，不会收敛。如果学习率太小，模型将花费太长时间来收敛。

学习率需要和训练周期，batch size 大小以及优化方法联系在一起考虑。

<img aligin="center" src="/img/learning.png" />



## 损失值

> 在深度学习中，损失值（Loss）是用来衡量模型预测结果与真实标签之间差异的一个指标。损失值越小，模型的预测结果与真实标签之间的差异就越小，模型的性能也就越好。
>
> 损失值通常是由损失函数（Loss Function）计算得到的。损失函数是用来衡量模型预测结果与真实标签之间差异的一个函数，其输入为模型的预测结果和真实标签，输出为一个标量，即损失值。在深度学习中，常见的损失函数包括均方误差（Mean Squared Error）、交叉熵（Cross Entropy）等。
>
> 在训练模型时，我们通常会将损失值作为优化目标，通过不断调整模型参数来使损失值最小化。具体地，我们会使用梯度下降（Gradient Descent）等优化算法来更新模型参数，使损失值不断减小，直到达到一个满意的水平。
>
> 需要注意的是，损失值只是用来衡量模型预测结果与真实标签之间差异的一个指标，它并不一定能完全反映模型的性能。例如，当模型过拟合时，损失值可能会很小，但是模型的泛化能力却很差。因此，在评估模型性能时，我们还需要结合其他指标，例如准确率、召回率等来进行综合评估。
>
> 总之，损失值是深度学习中用来衡量模型预测结果与真实标签之间差异的一个指标，其大小越小，模型的性能越好。在训练模型时，我们通常会将损失值作为优化目标，并通过不断调整模型参数来使其最小化。



### 在深度学习中，训练模型的过程通常被划分为多个步骤。其中，epoch、step、batch size以及batch是四个非常重要的概念。本文将详细介绍它们之间的关系。

## epoch

> 首先，我们来了解一下epoch的概念。Epoch指的是训练模型时使用整个训练集的次数。在每个epoch中，模型会对整个训练集进行一次完整的训练。通常情况下，一个epoch包含多个step。

## step

> 接下来，我们来了解一下step的概念。Step指的是每个epoch中模型更新参数的次数。在每个step中，模型会根据当前的batch size从训练集中取出相应数量的数据进行训练。每次完成一个step后，模型会更新一次参数。

## batch size

> 然后，我们来了解一下batch size的概念。Batch size指的是每次训练时从训练集中取出的数据数量。batch size的大小会影响训练的速度和模型的性能。
>
> Batch Size决定的是下降的方向。
>
> 1、Batch_Size 太小，可能导致算法不收敛；训练速度越慢，但是内存占用也会相应减少。
> 2、随着Batch_Size 增大，处理相同数据量的速度越快，但是会占用更多的内存。
> 3、随着Batch_Size 增大，达到相同精度所需要的epoch 数量越来越多。
> 4、由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。
> 5、由于最终收敛精度会陷入不同的局部极值，因此Batch_Size 增大到某些时候，达到最终收敛精度上的最优。
> 因此batch_size的值不能太大也不能太小，当batch_size慢慢增大到某一个值的时候，模型的性能也会慢慢的增强达到一个峰值，但是，当batch_size在继续增大的时候模型的性能会逐渐下降。并且内存也不能加载那么大的batch_size值。但是，如果batch_size调小的话，就不能充分利用电脑内存，并且小批量的batch_size其下降的方向不能很好代表样本下降的方向。因此，我们要在合理的范围内增加Btach_size的值。



## batch

> 最后，我们来了解一下batch的概念。Batch指的是每个step中从训练集中取出的数据。一个batch包含的数据数量由batch size决定。当batch size为1时，每个batch只包含一个数据。当batch size大于1时，每个batch包含多个数据。

综上所述，epoch、step、batch size以及batch是深度学习中非常重要的概念。它们之间的关系可以总结如下：每个epoch包含多个step，每个step使用batch size个数据进行训练，每个batch包含batch size个数据。在训练模型时，我们可以根据数据集的大小、内存大小、训练速度等因素来选择合适的epoch、step、batch size以及batch。。

---
## 相关阅读
- [超参数模型训练](https://developer.aliyun.com/article/1147950)

