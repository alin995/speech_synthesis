# transformerså­¦ä¹ ï¼ˆä¸ƒï¼‰å­¦ä¹ ç¬”è®°

---

Transformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚å®ƒçš„è®¾è®¡çµæ„Ÿæ¥è‡ªäºå¤§è„‘ç¥ç»å…ƒçš„è¿æ¥æ–¹å¼ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»å¤§è„‘çš„å¤„ç†è¿‡ç¨‹ã€‚Transformer æ¨¡å‹ç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨å°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºå‘é‡ï¼Œè€Œè§£ç å™¨å°†è¯¥å‘é‡è½¬æ¢ä¸ºç›®æ ‡åºåˆ—ã€‚

Transformer çš„æ ¸å¿ƒæ€æƒ³æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¯ä¸ªä½ç½®çš„æ³¨æ„åŠ›æƒé‡å–å†³äºå‘¨å›´çš„ä½ç½®å’Œè‡ªèº«çš„å†å²ä¿¡æ¯ã€‚è¿™ç§æœºåˆ¶ä½¿å¾— Transformer èƒ½å¤Ÿæ•æ‰åºåˆ—ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

Transformer è¿˜æœ‰è®¸å¤šä¼˜åŒ–æŠ€å·§ã€‚ä¾‹å¦‚ï¼ŒBERT æ¨¡å‹é‡‡ç”¨äº†é¢„è®­ç»ƒå’Œåè®­ç»ƒæŠ€æœ¯ï¼Œå³åœ¨è®­ç»ƒä¹‹å‰å…ˆè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†è¿›è¡Œå¾®è°ƒå’Œè®­ç»ƒã€‚æ­¤å¤–ï¼ŒTransformer è¿˜é‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯ï¼Œå³åœ¨åŒä¸€æ¨¡å‹ä¸­åŒæ—¶è®­ç»ƒå¤šä¸ªä»»åŠ¡ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

[Transformers](https://huggingface.co/docs/transformers/index) æ˜¯ç”± [Hugging Face](https://huggingface.co/) å¼€å‘çš„ä¸€ä¸ª NLP åŒ…ï¼Œæ”¯æŒåŠ è½½ç›®å‰ç»å¤§éƒ¨åˆ†çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚

------

# 1. å¼€ç®±å³ç”¨çš„ pipelines

Transformers åº“å°†ç›®å‰çš„ NLP ä»»åŠ¡å½’çº³ä¸ºå‡ ä¸‹å‡ ç±»ï¼š

- **æ–‡æœ¬åˆ†ç±»ï¼š**ä¾‹å¦‚æƒ…æ„Ÿåˆ†æã€å¥å­å¯¹å…³ç³»åˆ¤æ–­ç­‰ï¼›
- **å¯¹æ–‡æœ¬ä¸­çš„è¯è¯­è¿›è¡Œåˆ†ç±»ï¼š**ä¾‹å¦‚è¯æ€§æ ‡æ³¨ (POS)ã€å‘½åå®ä½“è¯†åˆ« (NER) ç­‰ï¼›
- **æ–‡æœ¬ç”Ÿæˆï¼š**ä¾‹å¦‚å¡«å……é¢„è®¾çš„æ¨¡æ¿ (prompt)ã€é¢„æµ‹æ–‡æœ¬ä¸­è¢«é®æ©æ‰ (masked) çš„è¯è¯­ï¼›
- **ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼š**ä¾‹å¦‚æ ¹æ®ç»™å®šçš„é—®é¢˜ä»ä¸€æ®µæ–‡æœ¬ä¸­æŠ½å–å‡ºå¯¹åº”çš„ç­”æ¡ˆï¼›
- **æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆæ–°çš„å¥å­ï¼š**ä¾‹å¦‚æ–‡æœ¬ç¿»è¯‘ã€è‡ªåŠ¨æ‘˜è¦ç­‰ã€‚

Transformers åº“æœ€åŸºç¡€çš„å¯¹è±¡å°±æ˜¯ `pipeline()` å‡½æ•°ï¼Œå®ƒå°è£…äº†é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„å‰å¤„ç†å’Œåå¤„ç†ç¯èŠ‚ã€‚æˆ‘ä»¬åªéœ€è¾“å…¥æ–‡æœ¬ï¼Œå°±èƒ½å¾—åˆ°é¢„æœŸçš„ç­”æ¡ˆã€‚ç›®å‰å¸¸ç”¨çš„ [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) æœ‰ï¼š

- `feature-extraction` ï¼ˆè·å¾—æ–‡æœ¬çš„å‘é‡åŒ–è¡¨ç¤ºï¼‰
- `fill-mask` ï¼ˆå¡«å……è¢«é®ç›–çš„è¯ã€ç‰‡æ®µï¼‰
- `ner`ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰
- `question-answering` ï¼ˆè‡ªåŠ¨é—®ç­”ï¼‰/ï¼ˆé˜…è¯»ç†è§£ï¼‰
- `sentiment-analysis` ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰
- `summarization` ï¼ˆè‡ªåŠ¨æ‘˜è¦ï¼‰
- `text-generation` ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰
- `translation` ï¼ˆæœºå™¨ç¿»è¯‘ï¼‰
- `zero-shot-classification` ï¼ˆé›¶è®­ç»ƒæ ·æœ¬åˆ†ç±»ï¼‰

æ¨¡å‹ç¤ºä¾‹ï¼š

```python3
from transformers import pipeline
question_answerer = pipeline("question-answering")
```

ç¬¬äºŒæ­¥ï¼Œå°†æ–‡ç« å’Œé—®é¢˜è¾“å…¥pipelineï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ

```python3
from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline
model = AutoModelForQuestionAnswering.from_pretrained('uer/roberta-base-chinese-extractive-qa')
tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-chinese-extractive-qa')
```

æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸å‰é¢ç±»å‹ï¼Œä½†æ˜¯åœ¨è°ƒç”¨pipelineæ—¶ï¼Œéœ€è¦å°†æ¨¡å‹å’Œåˆ†è¯å™¨è¿›è¡ŒæŒ‡å®šï¼Œä»£ç å¦‚ä¸‹ï¼š

```python3
zh_qa = pipeline("question-answering", model=model, tokenizer=tokenizer)
QA_input = {'question': "è‘—åè¯—æ­Œã€Šå‡å¦‚ç”Ÿæ´»æ¬ºéª—äº†ä½ ã€‹çš„ä½œè€…æ˜¯",'context': "æ™®å¸Œé‡‘ä»é‚£é‡Œå­¦ä¹ äººæ°‘çš„è¯­è¨€ï¼Œå¸å–äº†è®¸å¤šæœ‰ç›Šçš„å…»æ–™ï¼Œè¿™ä¸€åˆ‡å¯¹æ™®å¸Œé‡‘åæ¥çš„åˆ›ä½œäº§ç”Ÿäº†å¾ˆå¤§çš„å½±å“ã€‚è¿™ä¸¤å¹´é‡Œï¼Œæ™®å¸Œé‡‘åˆ›ä½œäº†ä¸å°‘ä¼˜ç§€çš„ä½œå“ï¼Œå¦‚ã€Šå›šå¾’ã€‹ã€ã€Šè‡´å¤§æµ·ã€‹ã€ã€Šè‡´å‡¯æ©ã€‹å’Œã€Šå‡å¦‚ç”Ÿæ´»æ¬ºéª—äº†ä½ ã€‹ç­‰å‡ åé¦–æŠ’æƒ…è¯—ï¼Œå™äº‹è¯—ã€ŠåŠªæ—ä¼¯çˆµã€‹ï¼Œå†å²å‰§ã€Šé²é‡Œæ–¯Â·æˆˆéƒ½è¯ºå¤«ã€‹ï¼Œä»¥åŠã€Šå¶ç”«ç›–å°¼Â·å¥¥æ¶…é‡‘ã€‹å‰å…­ç« ã€‚"}
zh_qa(QA_input)

{'score': 0.976427278518677,'start':0,'end':3,'answer':'æ™®å¸Œé‡‘'}
```

#### pipeline èƒŒååšäº†ä»€ä¹ˆï¼š

1. é¢„å¤„ç† (preprocessing)ï¼Œå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥æ¥å—çš„è¾“å…¥æ ¼å¼ï¼›
2. å°†å¤„ç†å¥½çš„è¾“å…¥é€å…¥æ¨¡å‹ï¼›
3. å¯¹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œåå¤„ç† (postprocessing)ï¼Œå°†å…¶è½¬æ¢ä¸ºäººç±»æ–¹ä¾¿é˜…è¯»çš„æ ¼å¼ã€‚


<img align=â€œcenterâ€ src="/img/pipeline.png"/>


### ä½¿ç”¨åˆ†è¯å™¨è¿›è¡Œé¢„å¤„ç†

å› ä¸ºç¥ç»ç½‘ç»œæ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†æ–‡æœ¬ï¼Œå› æ­¤é¦–å…ˆéœ€è¦é€šè¿‡**é¢„å¤„ç†**ç¯èŠ‚å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨æ¯ä¸ªæ¨¡å‹å¯¹åº”çš„åˆ†è¯å™¨ (tokenizer) æ¥è¿›è¡Œï¼š

1. å°†è¾“å…¥åˆ‡åˆ†ä¸ºè¯è¯­ã€å­è¯æˆ–è€…ç¬¦å·ï¼ˆä¾‹å¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç»Ÿç§°ä¸º **tokens**ï¼›
2. æ ¹æ®æ¨¡å‹çš„è¯è¡¨å°†æ¯ä¸ª token æ˜ å°„åˆ°å¯¹åº”çš„ token ç¼–å·ï¼ˆå°±æ˜¯ä¸€ä¸ªæ•°å­—ï¼‰ï¼›
3. æ ¹æ®æ¨¡å‹çš„éœ€è¦ï¼Œæ·»åŠ ä¸€äº›é¢å¤–çš„è¾“å…¥ã€‚

æˆ‘ä»¬å¯¹è¾“å…¥æ–‡æœ¬çš„é¢„å¤„ç†éœ€è¦ä¸æ¨¡å‹è‡ªèº«é¢„è®­ç»ƒæ—¶çš„æ“ä½œå®Œå…¨ä¸€è‡´ï¼Œåªæœ‰è¿™æ ·æ¨¡å‹æ‰å¯ä»¥æ­£å¸¸åœ°å·¥ä½œã€‚æ³¨æ„ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ç‰¹å®šçš„é¢„å¤„ç†æ“ä½œï¼Œå¦‚æœå¯¹è¦ä½¿ç”¨çš„æ¨¡å‹ä¸ç†Ÿæ‚‰ï¼Œå¯ä»¥é€šè¿‡ [Model Hub](https://huggingface.co/models) æŸ¥è¯¢ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ `AutoTokenizer` ç±»å’Œå®ƒçš„ `from_pretrained()` å‡½æ•°ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨æ ¹æ®æ¨¡å‹ checkpoint åç§°æ¥è·å–å¯¹åº”çš„åˆ†è¯å™¨ã€‚

æƒ…æ„Ÿåˆ†æ pipeline çš„é»˜è®¤ checkpoint æ˜¯ [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)ï¼Œä¸‹é¢æˆ‘ä»¬æ‰‹å·¥ä¸‹è½½å¹¶è°ƒç”¨å…¶åˆ†è¯å™¨ï¼š

```
from transformers import AutoTokenizer
checkpoint = "distilbert-base-uncased-finetuned-sst-2-english" tokenizer = AutoTokenizer.from_pretrained(checkpoint) 
raw_inputs = [    "I've been waiting for a HuggingFace course my whole life.",    "I hate this so much!",
] 
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt") print(inputs)
```

```
{    
'input_ids': tensor([      
		[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],       
    [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,          
    0,     0,     0,     0,     0,     0]  
  ]),   
 'attention_mask': tensor([    
		[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],     
    [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]   
  ])
}
```

å¯ä»¥çœ‹åˆ°ï¼Œè¾“å‡ºä¸­åŒ…å«ä¸¤ä¸ªé”® `input_ids` å’Œ `attention_mask`ï¼Œå…¶ä¸­ `input_ids` å¯¹åº”åˆ†è¯ä¹‹åçš„ tokens æ˜ å°„åˆ°çš„æ•°å­—ç¼–å·åˆ—è¡¨ï¼Œè€Œ `attention_mask` åˆ™æ˜¯ç”¨æ¥æ ‡è®°å“ªäº› tokens æ˜¯è¢«å¡«å……çš„ï¼ˆè¿™é‡Œâ€œ1â€è¡¨ç¤ºæ˜¯åŸæ–‡ï¼Œâ€œ0â€è¡¨ç¤ºæ˜¯å¡«å……å­—ç¬¦ï¼‰ã€‚

### å°†é¢„å¤„ç†å¥½çš„è¾“å…¥é€å…¥æ¨¡å‹

é¢„è®­ç»ƒæ¨¡å‹çš„ä¸‹è½½æ–¹å¼å’Œåˆ†è¯å™¨ (tokenizer) ç±»ä¼¼ï¼ŒTransformers åŒ…æä¾›äº†ä¸€ä¸ª `AutoModel` ç±»å’Œå¯¹åº”çš„ `from_pretrained()` å‡½æ•°ã€‚ä¸‹é¢æˆ‘ä»¬æ‰‹å·¥ä¸‹è½½è¿™ä¸ª distilbert-base æ¨¡å‹ï¼š

```
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

é¢„è®­ç»ƒæ¨¡å‹çš„æœ¬ä½“åªåŒ…å«åŸºç¡€çš„ Transformer æ¨¡å—ï¼Œå¯¹äºç»™å®šçš„è¾“å…¥ï¼Œå®ƒä¼šè¾“å‡ºä¸€äº›ç¥ç»å…ƒçš„å€¼ï¼Œç§°ä¸º hidden states æˆ–è€…ç‰¹å¾ (features)ã€‚å¯¹äº NLP æ¨¡å‹æ¥è¯´ï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯æ–‡æœ¬çš„é«˜ç»´è¯­ä¹‰è¡¨ç¤ºã€‚è¿™äº› hidden states é€šå¸¸ä¼šè¢«è¾“å…¥åˆ°å…¶ä»–çš„æ¨¡å‹éƒ¨åˆ†ï¼ˆç§°ä¸º headï¼‰ï¼Œä»¥å®Œæˆç‰¹å®šçš„ä»»åŠ¡ï¼Œä¾‹å¦‚é€å…¥åˆ°åˆ†ç±»å¤´ä¸­å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

å…¶å®å‰é¢æˆ‘ä»¬ä¸¾ä¾‹çš„æ‰€æœ‰ pipelines éƒ½å…·æœ‰ç±»ä¼¼çš„æ¨¡å‹ç»“æ„ï¼Œåªæ˜¯æ¨¡å‹çš„æœ€åä¸€éƒ¨åˆ†ä¼šä½¿ç”¨ä¸åŒçš„ head ä»¥å®Œæˆå¯¹åº”çš„ä»»åŠ¡ã€‚

<img align=â€œcenterâ€ src="/img/modules.png"/>

Transformer æ¨¡å—çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç»´åº¦ä¸º (Batch size, Sequence length, Hidden size) çš„ä¸‰ç»´å¼ é‡ï¼Œå…¶ä¸­ Batch size è¡¨ç¤ºæ¯æ¬¡è¾“å…¥çš„æ ·æœ¬ï¼ˆæ–‡æœ¬åºåˆ—ï¼‰æ•°é‡ï¼Œå³æ¯æ¬¡è¾“å…¥å¤šå°‘ä¸ªå¥å­ï¼Œä¸Šä¾‹ä¸­ä¸º 2ï¼›Sequence length è¡¨ç¤ºæ–‡æœ¬åºåˆ—çš„é•¿åº¦ï¼Œå³æ¯ä¸ªå¥å­è¢«åˆ†ä¸ºå¤šå°‘ä¸ª tokenï¼Œä¸Šä¾‹ä¸­ä¸º 16ï¼›Hidden size è¡¨ç¤ºæ¯ä¸€ä¸ª token ç»è¿‡æ¨¡å‹ç¼–ç åçš„è¾“å‡ºå‘é‡ï¼ˆè¯­ä¹‰è¡¨ç¤ºï¼‰çš„ç»´åº¦ã€‚

> é¢„è®­ç»ƒæ¨¡å‹ç¼–ç åçš„è¾“å‡ºå‘é‡çš„ç»´åº¦é€šå¸¸éƒ½å¾ˆå¤§ï¼Œä¾‹å¦‚ Bert æ¨¡å‹ base ç‰ˆæœ¬çš„è¾“å‡ºä¸º 768 ç»´ï¼Œä¸€äº›å¤§æ¨¡å‹çš„è¾“å‡ºç»´åº¦ä¸º 3072 ç”šè‡³æ›´é«˜ã€‚
>
> æˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºè¿™é‡Œä½¿ç”¨çš„ distilbert-base æ¨¡å‹çš„è¾“å‡ºç»´åº¦ï¼š
>
> ```
> from transformers import AutoTokenizer, AutoModel
> 
> checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
> model = AutoModel.from_pretrained(checkpoint)
> 
> raw_inputs = [
>     "I've been waiting for a HuggingFace course my whole life.",
>     "I hate this so much!",
> ]
> inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
> outputs = model(**inputs)
> print(outputs.last_hidden_state.shape)
> torch.Size([2, 16, 768])
> ```

# 2. æ¨¡å‹ Models 

å¸¸ç”¨çš„æ¨¡å‹ä¸€èˆ¬åˆ†ä¸ºä¸‰ç§ï¼šè‡ªå›å½’æ¨¡å‹ã€è‡ªç¼–ç æ¨¡å‹å’Œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚

- **è‡ªå›å½’æ¨¡å‹**é‡‡ç”¨ç»å…¸çš„è¯­è¨€æ¨¡å‹ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå³ç»™å‡ºä¸Šæ–‡ï¼Œé¢„æµ‹ä¸‹æ–‡ï¼Œå¯¹åº”åŸå§‹Transformeræ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†ï¼Œå…¶ä¸­æœ€ç»å…¸çš„æ¨¡å‹æ˜¯GPTã€‚ç”±äºè‡ªç¼–ç å™¨åªèƒ½çœ‹åˆ°ä¸Šæ–‡è€Œæ— æ³•çœ‹åˆ°ä¸‹æ–‡çš„ç‰¹ç‚¹ï¼Œæ¨¡å‹ä¸€èˆ¬ä¼šç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ä»»åŠ¡ã€‚
- **è‡ªç¼–ç æ¨¡å‹**åˆ™é‡‡ç”¨å¥å­é‡å»ºçš„ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå³é¢„å…ˆé€šè¿‡æŸç§æ–¹å¼ç ´åå¥å­ï¼Œå¯èƒ½æ˜¯æ©ç ï¼Œå¯èƒ½æ˜¯æ‰“ä¹±é¡ºåºï¼Œå¸Œæœ›æ¨¡å‹å°†è¢«ç ´åçš„éƒ¨åˆ†è¿˜åŸï¼Œå¯¹åº”åŸå§‹Transformeræ¨¡å‹çš„ç¼–ç å™¨éƒ¨åˆ†ï¼Œå…¶ä¸­æœ€ç»å…¸çš„æ¨¡å‹æ˜¯BERTã€‚ä¸è‡ªå›å½’æ¨¡å‹ä¸åŒï¼Œæ¨¡å‹æ—¢å¯ä»¥çœ‹åˆ°ä¸Šæ–‡ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥çœ‹åˆ°ä¸‹æ–‡ä¿¡æ¯ï¼Œç”±äºè¿™æ ·çš„ç‰¹ç‚¹ï¼Œè‡ªç¼–ç æ¨¡å‹å¾€å¾€ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€é˜…è¯»ç†è§£ç­‰ã€‚ï¼ˆæ­¤å¤–ï¼Œè¿™é‡Œéœ€è¦æ³¨æ„ï¼Œè‡ªç¼–ç æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„å”¯ä¸€åŒºåˆ†å…¶å®æ˜¯åœ¨äºé¢„è®­ç»ƒæ—¶çš„ä»»åŠ¡ï¼Œè€Œä¸æ˜¯æ¨¡å‹ç»“æ„ã€‚ï¼‰
- **åºåˆ—åˆ°åºåˆ—æ¨¡å‹**åˆ™æ˜¯åŒæ—¶ä½¿ç”¨äº†åŸå§‹çš„ç¼–ç å™¨ä¸è§£ç å™¨ï¼Œæœ€ç»å…¸çš„æ¨¡å‹ä¾¿æ˜¯T5ã€‚ä¸ç»å…¸çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ç±»ä¼¼ï¼Œè¿™ç§æ¨¡å‹æœ€è‡ªç„¶çš„åº”ç”¨ä¾¿æ˜¯æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ï¼Œäº‹å®ä¸ŠåŸºæœ¬æ‰€æœ‰çš„NLPä»»åŠ¡éƒ½å¯ä»¥é€šè¿‡åºåˆ—åˆ°åºåˆ—è§£å†³ã€‚

ä¸‹è¡¨ä¸­æ€»ç»“äº†ä»¥ä¸Šä¸‰ç§ç±»å‹æ¨¡å‹çš„å¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠé€‚åˆå¤„ç†çš„è§£å†³çš„ä»»åŠ¡ã€‚

| æ¨¡å‹ç±»å‹       | å¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹                    | é€‚ç”¨ä»»åŠ¡                         |
| -------------- | --------------------------------- | -------------------------------- |
| è‡ªå›å½’æ¨¡å‹     | CTRL, GPT, GPT-2, Transformer XL  | æ–‡æœ¬ç”Ÿæˆ                         |
| è‡ªç¼–ç æ¨¡å‹     | ALBERT, BERT, DistilBERT, RoBERTa | æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é˜…è¯»ç†è§£ |
| åºåˆ—åˆ°åºåˆ—æ¨¡å‹ | BART, T5, Marian, mBART           | æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘               |

ä»¥åŠ è½½bert-base-chineseæ¨¡å‹ä¸ºä¾‹ï¼Œä»£ç å¦‚ä¸‹ã€‚

```python3
from transformers import BertModel
model = BertModel.from_pretrained("bert-base-chinese")
```

> `Model.from_pretrained()` ä¼šè‡ªåŠ¨ç¼“å­˜ä¸‹è½½çš„æ¨¡å‹æƒé‡ï¼Œé»˜è®¤ä¿å­˜åˆ° *~/.cache/huggingface/transformers*ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ HF_HOME ç¯å¢ƒå˜é‡è‡ªå®šä¹‰ç¼“å­˜ç›®å½•ã€‚

æ‰€æœ‰å­˜å‚¨åœ¨ [Model Hub](https://huggingface.co/models) ä¸Šçš„æ¨¡å‹éƒ½èƒ½å¤Ÿé€šè¿‡ `Model.from_pretrained()` åŠ è½½ï¼Œåªéœ€è¦ä¼ é€’å¯¹åº” checkpoint çš„åç§°ã€‚å½“ç„¶äº†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å…ˆå°†æ¨¡å‹ä¸‹è½½ä¸‹æ¥ï¼Œç„¶åå°†æœ¬åœ°è·¯å¾„ä¼ ç»™ `Model.from_pretrained()`ï¼Œæ¯”å¦‚åŠ è½½ä¸‹è½½å¥½çš„ [Bert-base æ¨¡å‹](https://huggingface.co/bert-base-cased)ï¼š

```python
from transformers import BertModel

model = BertModel.from_pretrained("./models/bert/")
```

éƒ¨åˆ†æ¨¡å‹çš„ Hub é¡µé¢ä¸­ä¼šåŒ…å«å¾ˆå¤šæ–‡ä»¶ï¼Œæˆ‘ä»¬é€šå¸¸åªéœ€è¦ä¸‹è½½æ¨¡å‹å¯¹åº”çš„ *config.json* å’Œ *pytorch_model.bin*ï¼Œä»¥åŠåˆ†è¯å™¨å¯¹åº”çš„ *tokenizer.json*ã€*tokenizer_config.json* å’Œ *vocab.txt*ã€‚

### ä¿å­˜æ¨¡å‹

ä¿å­˜æ¨¡å‹ä¸åŠ è½½æ¨¡å‹ç±»ä¼¼ï¼Œåªéœ€è¦è°ƒç”¨ `Model.save_pretrained()` å‡½æ•°ã€‚ä¾‹å¦‚ä¿å­˜åŠ è½½çš„ BERT æ¨¡å‹ï¼š

```
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")
model.save_pretrained("./models/bert-base-cased/")
```

è¿™ä¼šåœ¨ä¿å­˜è·¯å¾„ä¸‹åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼š

- config.jsonï¼šæ¨¡å‹é…ç½®æ–‡ä»¶ï¼Œé‡Œé¢åŒ…å«æ„å»ºæ¨¡å‹ç»“æ„çš„å¿…è¦å‚æ•°ï¼›
- pytorch_model.binï¼šåˆç§°ä¸º state dictionaryï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰æƒé‡ã€‚

è¿™ä¸¤ä¸ªæ–‡ä»¶ç¼ºä¸€ä¸å¯ï¼Œé…ç½®æ–‡ä»¶è´Ÿè´£è®°å½•æ¨¡å‹çš„**ç»“æ„**ï¼Œæ¨¡å‹æƒé‡è®°å½•æ¨¡å‹çš„**å‚æ•°**ã€‚æˆ‘ä»¬è‡ªå·±ä¿å­˜çš„æ¨¡å‹åŒæ ·å¯ä»¥é€šè¿‡ `Model.from_pretrained()` åŠ è½½ï¼Œåªéœ€è¦ä¼ é€’ä¿å­˜ç›®å½•çš„è·¯å¾„ã€‚

# 3.åˆ†è¯å™¨  Tokenizer

å› ä¸ºç¥ç»ç½‘ç»œæ¨¡å‹ä¸èƒ½ç›´æ¥å¤„ç†æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æ•°å­—ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º**ç¼–ç  (Encoding)**ï¼šå…ˆä½¿ç”¨åˆ†è¯å™¨ (Tokenizers) å°†æ–‡æœ¬æŒ‰è¯ã€å­è¯ã€ç¬¦å·åˆ‡åˆ†ä¸º tokensï¼›ç„¶åå°† tokens æ˜ å°„åˆ°å¯¹åº”çš„ token ç¼–å·ï¼ˆtoken IDsï¼‰ã€‚

ï¼ˆ1ï¼‰åˆ†è¯ï¼šä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ï¼ˆå­—ã€å­—è¯ï¼‰ï¼›

ï¼ˆ2ï¼‰æ„å»ºè¯å…¸ï¼šæ ¹æ®æ•°æ®é›†åˆ†è¯çš„ç»“æœï¼Œæ„å»ºè¯å…¸æ˜ å°„ï¼ˆè¿™ä¸€æ­¥å¹¶ä¸ç»å¯¹ï¼Œå¦‚æœé‡‡ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼Œè¯å…¸æ˜ å°„è¦æ ¹æ®è¯å‘é‡æ–‡ä»¶è¿›è¡Œå¤„ç†ï¼‰ï¼›

ï¼ˆ3ï¼‰æ•°æ®è½¬æ¢ï¼šæ ¹æ®æ„å»ºå¥½çš„è¯å…¸ï¼Œå°†åˆ†è¯å¤„ç†åçš„æ•°æ®åšæ˜ å°„ï¼Œå°†æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼›

ï¼ˆ4ï¼‰æ•°æ®å¡«å……ä¸æˆªæ–­ï¼šåœ¨ä»¥batchè¾“å…¥åˆ°æ¨¡å‹çš„æ–¹å¼ä¸­ï¼Œéœ€è¦å¯¹è¿‡çŸ­çš„æ•°æ®è¿›è¡Œå¡«å……ï¼Œè¿‡é•¿çš„æ•°æ®è¿›è¡Œæˆªæ–­ï¼Œä¿è¯æ•°æ®é•¿åº¦ç¬¦åˆæ¨¡å‹èƒ½æ¥å—çš„èŒƒå›´ï¼ŒåŒæ—¶batchå†…çš„æ•°æ®ç»´åº¦å¤§å°ä¸€è‡´ã€‚

åœ¨ä»¥å¾€çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨ï¼Œå¹¶è‡ªè¡Œå®ç°æ„å»ºè¯å…¸ä¸è½¬æ¢çš„å·¥ä½œã€‚ä½†æ˜¯åœ¨transformerså·¥å…·åŒ…ä¸­ï¼Œæ— éœ€å†è¿™èˆ¬å¤æ‚ï¼Œåªéœ€è¦å€ŸåŠ©Tokenizeræ¨¡å—ä¾¿å¯ä»¥å¿«é€Ÿçš„å®ç°ä¸Šè¿°å…¨éƒ¨å·¥ä½œï¼Œå®ƒçš„åŠŸèƒ½å°±æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯ä»¥å¤„ç†çš„æ•°æ®ã€‚Tokenizerå·¥å…·åŒ…æ— éœ€é¢å¤–å®‰è£…ï¼Œä¼šéšç€transformersä¸€èµ·å®‰è£…ã€‚ä¸‹é¢ï¼Œæ¼”ç¤ºä¸€ä¸‹å…·ä½“è¯¥å¦‚ä½•ä½¿ç”¨Tokenizerã€‚

#### åŠ è½½åˆ†è¯å™¨

```python3
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
```

å¥å­åˆ†è¯

ä½¿ç”¨tokenizeæ–¹æ³•è¿›è¡Œåˆ†è¯ï¼Œä»£ç å¦‚ä¸‹ã€‚å¯ä»¥çœ‹åˆ°ï¼Œbert-base-chineseåˆ†è¯æ˜¯æŒ‰ç…§å­—æ¥åˆ†çš„ã€‚

```python3
sen = "å¼±å°çš„æˆ‘ä¹Ÿæœ‰å¤§æ¢¦æƒ³"
tokens = tokenizer.tokenize(sen)
tokens

ç»“æœï¼š['å¼±','å°','çš„','æˆ‘','ä¹Ÿ','æœ‰','å¤§','æ¢¦','æƒ³']
```

### æŸ¥çœ‹è¯å…¸

åˆ†è¯ä¹‹åï¼Œåº”è¯¥æ„å»ºè¯å…¸ï¼Œä½†æ˜¯æ­£å¦‚å‰é¢æ‰€è¨€ï¼ŒTokenzieræ˜¯éšç€é¢„è®­ç»ƒæ¨¡å‹ä¸€èµ·äº§ç”Ÿçš„ï¼Œå› æ­¤è¯å…¸å·²ç»é¢„å…ˆæ„å»ºå¥½äº†ï¼Œæ— éœ€å†æ¬¡æ„å»ºã€‚å…³äºè¯å…¸çš„å…·ä½“å†…å®¹ï¼Œå¯ä»¥é€šè¿‡vocabè¿›è¡ŒæŸ¥çœ‹ã€‚

```
Tokenizer.vocab

ç»“æœï¼š
'è¡Œ'ï¼š 6121ï¼Œ
â€˜è²‚â€™ï¼š6503
...
```

### **è¯åºåˆ—è½¬æ•°å­—åºåˆ—**

Tokenizeræä¾›äº†æ›´åŠ ä¾¿æ·çš„encodeæ–¹æ³•ï¼š

```text
ids = tokenizer.encode(sen)
ids

ç»“æœï¼šã€233ï¼Œ101ï¼Œ102ï¼Œ2207ï¼Œ738ï¼Œ1902ã€‘
```

### å¡«å……ä¸æˆªæ–­

å€ŸåŠ©encodeæ–¹æ³•ï¼Œè¿˜å¯ä»¥å¾ˆæ–¹ä¾¿çš„åšåˆ°å¯¹æ•°æ®çš„å¡«å……ä¸æˆªæ–­ï¼Œåªéœ€è¦æˆ‘ä»¬æŒ‡å®šå¯¹åº”çš„å‚æ•°å³å¯ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python3
# å¡«å……
ids = tokenizer.encode(sen, padding="max_length", max_length=10)
ids

ç»“æœï¼šã€233ï¼Œ101ï¼Œ102ï¼Œ2207ï¼Œ738ï¼Œ1902ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ã€‘
```

```python3
# è£å‰ª
ids = tokenizer.encode(sen, max_length=5, truncation=True)
ids

ç»“æœï¼šã€233ï¼Œ101ï¼Œ102ï¼Œ2207ï¼Œ738ã€‘
```

### **attention_mask ä¸ token_type_id**

è¿˜æ²¡ç»“æŸï¼Œæ•°æ®è¦èƒ½å¤Ÿè¾“å…¥transformersæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿˜éœ€è¦æ„å»ºattention_maskå’Œtoken_type_idè¿™ä¸¤ä¸ªé¢å¤–çš„è¾“å…¥ï¼Œåˆ†åˆ«ç”¨äºæ ‡è®°çœŸå®çš„è¾“å…¥ä¸ç‰‡æ®µç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢è¿™æ®µä»£ç å®ç°

```python3
ids = tokenizer.encode(sen, padding="max_length", max_length=15)
attention_mask = [1 if idx != 0 else 0 for idx in ids]
token_type_ids = [0] * len(ids)
attention_mask, token_type_ids

ç»“æœï¼š
(ã€1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ã€‘,
ã€0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ã€‘)

'token_type_ids'ï¼Œ åŒºåˆ†ä¸¤ä¸ªå¥å­çš„ç¼–ç 
'attention_mask', æŒ‡å®šå¯¹å“ªäº›è¯è¿›è¡Œself-Attentionæ“ä½œ
```

---

>huggingfaceçš„å®˜æ–¹ç½‘ç«™ï¼š[http://www.huggingface.co.](https://link.zhihu.com/?target=http%3A//www.huggingface.co./) åœ¨è¿™é‡Œä¸»è¦æœ‰ä»¥ä¸‹å¤§å®¶éœ€è¦çš„èµ„æºã€‚
>
>1. Datasetsï¼šæ•°æ®é›†ï¼Œä»¥åŠæ•°æ®é›†çš„ä¸‹è½½åœ°å€
>2. Modelsï¼šå„ä¸ªé¢„è®­ç»ƒæ¨¡å‹
>3. courseï¼šå…è´¹çš„nlpè¯¾ç¨‹ï¼Œå¯æƒœéƒ½æ˜¯è‹±æ–‡çš„
>4. docsï¼šæ–‡æ¡£
>
>[huggingfaceå®˜æ–¹æ•™ç¨‹](https://huggingface.co/docs/transformers/model_doc/bert)
>
>[å®‰è£…Huggfaceåº“](https://huggingface.co/transformers/installation.html#)(éœ€è¦é¢„å…ˆå®‰è£…pytorch)

------

# Task

transformers é¢„è®­ç»ƒçš„ä»»åŠ¡åŒ…æ‹¬è¯­è¨€å»ºæ¨¡ã€æ©ç è¯­è¨€å»ºæ¨¡ã€ä¸‹ä¸€å¥é¢„æµ‹ã€è¿ç»­æ–‡æœ¬é¢„æµ‹ã€æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡ã€‚

1. è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼šä¾‹å¦‚BERTã€GPTã€ELMoç­‰ï¼Œè¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹å­¦ä¹ åˆ°ä¸€ä¸ªé€šç”¨çš„è¯­è¨€è¡¨ç¤ºå½¢å¼ï¼Œé€šå¸¸é‡‡ç”¨æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelingï¼‰å’Œä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNext Sentence Predictionï¼‰ç­‰æŠ€æœ¯ã€‚
2. é—®ç­”ç³»ç»Ÿé¢„è®­ç»ƒï¼šä¾‹å¦‚T5ã€UniLMç­‰ï¼Œè¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„é—®é¢˜å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå¯¹åº”çš„ç­”æ¡ˆï¼Œé€šå¸¸ä½¿ç”¨è‡ªç„¶è¯­è¨€ç”Ÿæˆå’Œé—®ç­”åŒ¹é…ç­‰æŠ€æœ¯ã€‚
3. åºåˆ—æ ‡æ³¨é¢„è®­ç»ƒï¼šä¾‹å¦‚BERT-CRFã€Transformer-CRFç­‰ï¼Œè¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹å­¦ä¹ åˆ°åºåˆ—çš„å†…éƒ¨ç»“æ„å’Œæ ‡ç­¾ï¼Œé€šå¸¸é‡‡ç”¨æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰ç­‰æŠ€æœ¯ã€‚
4. æœºå™¨ç¿»è¯‘é¢„è®­ç»ƒï¼šä¾‹å¦‚Marianã€Transformer-XLç­‰ï¼Œè¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç¿»è¯‘çš„è§„å¾‹å’Œæ¨¡å¼ï¼Œé€šå¸¸é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè‡ªå›å½’æ¨¡å‹ç­‰æŠ€æœ¯ã€‚
5. å›¾åƒç”Ÿæˆé¢„è®­ç»ƒï¼šä¾‹å¦‚DALL-Eã€CLIPç­‰ï¼Œè¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºç›¸åº”çš„å›¾åƒæˆ–è§†é¢‘ï¼Œé€šå¸¸ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç­‰æŠ€æœ¯ã€‚

------

### AutoModel

>æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©å·¥å…·ï¼Œå…¶èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„ä»»åŠ¡å’Œè¯­è¨€æ•°æ®ç±»å‹è‡ªåŠ¨é€‰æ‹©é€‚åˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬BERTã€GPTã€RoBERTaã€XLMç­‰ç­‰ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„é…ç½®ï¼Œä½¿ç”¨æˆ·å¯ä»¥æ›´åŠ æ–¹ä¾¿åœ°ä½¿ç”¨Transformersé¢„è®­ç»ƒæ¨¡å‹ã€‚
>
>ä½¿ç”¨AutoModelï¼Œç”¨æˆ·åªéœ€è¦æŒ‡å®šä»»åŠ¡åç§°ï¼ˆå¦‚åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ç­‰ï¼‰å’Œè¾“å…¥æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒç­‰ï¼‰ï¼ŒAutoModelå°±ä¼šè‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸€äº›è‡ªå®šä¹‰å‚æ•°é€‰é¡¹ï¼Œä»¥æ»¡è¶³ç”¨æˆ·ç‰¹å®šçš„éœ€æ±‚ã€‚å¦å¤–ï¼ŒAutoModelä¹Ÿæ”¯æŒä»é¢„è®­ç»ƒæ¨¡å‹ä¸­é€‰æ‹©æŒ‡å®šçš„å±‚æˆ–è€…å‡ ä½•è‡ªåŠ¨ç¼–ç å™¨ä¸­çš„å‡ ä½•è¡¨ç¤ºå±‚ã€‚å®ƒçš„è®¾è®¡ä½¿å¾—ä½¿ç”¨Transformerså¹³å°æ›´åŠ ç®€å•ï¼Œå‡å°‘äº†æ‰‹åŠ¨é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹å’Œè°ƒå‚çš„å¤æ‚æ€§å’Œå·¥ä½œé‡ã€‚



### AutoModelForCausalLM

> Transformersé¢„è®­ç»ƒçš„AutoModelForCausalLMæ˜¯ä¸€ç§ç”¨äºç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ï¼ˆGenerative Language Modelï¼‰çš„è‡ªåŠ¨åŒ–é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©å·¥å…·ã€‚é¢„æµ‹ä¸€ä¸ªsequenceä¹‹åçš„tokençš„ä»»åŠ¡ï¼Œåœ¨è¿™ç§æƒ…å¢ƒä¸‹ï¼Œæ¨¡å‹åªä¼šattend left contextï¼ˆmaskå·¦è¾¹çš„tokenï¼‰ã€‚è¿™æ ·çš„è®­ç»ƒè®¾ç½®ç‰¹åˆ«å…³æ³¨äºç”Ÿæˆä»»åŠ¡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ˜¯é€šè¿‡æŠ½æ ·è¾“å…¥sequenceå¾—åˆ°çš„æœ€åä¸€å±‚hidden stateçš„logitså¾—åˆ°çš„ã€‚
>
> è¯¥æ¨¡å‹æ ¹æ®è¾“å…¥çš„ä»»åŠ¡å’Œè¯­è¨€æ•°æ®ç±»å‹è‡ªåŠ¨é€‰æ‹©é€‚åˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚GPTå’ŒGPT-2ç­‰æ¨¡å‹ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„é…ç½®ã€‚AutoModelForCausalLMçš„ç›®æ ‡æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„APIï¼Œä»¥ç”Ÿæˆå…·æœ‰è¿ç»­æ€§çš„æ–‡æœ¬ã€‚
>
> ä¸å…¶ä»–ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼Œå®ƒçš„ä¼˜åŠ¿åœ¨äºå®ƒæ˜¯ä¸€ç§åŸºäºè‡ªå›å½’ï¼ˆAutoregressiveï¼‰çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨å‰ä¸€æ—¶åˆ»é¢„æµ‹åä¸€æ—¶åˆ»ï¼Œç”±æ­¤ç”Ÿæˆè¿ç»­æ€§æ–‡æœ¬ã€‚ æ‰€ä»¥ï¼ŒAutoModelForCausalLMä¸»è¦ç”¨äºåŸºäºè‡ªå›å½’çš„ç”Ÿæˆå¼ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”ŸæˆåŠå…¶å®ƒç”Ÿæˆå¼ä»»åŠ¡ã€‚æ­¤æ¨¡å‹å¯¹äºè¯¸å¦‚ç”µå½±è„šæœ¬ç”Ÿæˆï¼Œæœºå™¨ç¿»è¯‘ï¼Œç”»åƒç”Ÿæˆï¼ŒéŸ³ä¹ç”Ÿæˆç­‰ç”¨ä¾‹éå¸¸æœ‰ç”¨ã€‚

### AutoModelForImageClassification

> AutoModelForImageClassificationæ˜¯ä½¿ç”¨Hugging Face Transformersåº“ä¸­çš„è‡ªåŠ¨æ¨¡å‹é€‰æ‹©åŠŸèƒ½ï¼Œæ ¹æ®è¾“å…¥çš„æ•°æ®é›†å’Œä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é¢„è®­ç»ƒå›¾åƒåˆ†ç±»æ¨¡å‹ã€‚å®ƒå¯ä»¥å¤„ç†ä¸åŒçš„è¾“å…¥æ•°æ®ç±»å‹å’Œä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰ã€‚å…·ä½“è€Œè¨€ï¼ŒAutoModelForImageClassificationä¼šè‡ªåŠ¨é€‰æ‹©å¹¶åŠ è½½æœ€é€‚åˆè¾“å…¥æ•°æ®å’Œä»»åŠ¡ç±»å‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡å¯¹é½é¢„è®­ç»ƒæ¨¡å‹å’Œç›®æ ‡ä»»åŠ¡ï¼Œè¿›è¡Œå¾®è°ƒä»¥å®Œæˆå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚å®ƒå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†æ‰‹åŠ¨é€‰æ‹©å’Œé…ç½®æ¨¡å‹çš„å·¥ä½œé‡ã€‚

### AutoModelForImageSegmentation

>  AutoModelForImageSegmentationæ˜¯ä½¿ç”¨Hugging Face Transformersåº“ä¸­çš„è‡ªåŠ¨æ¨¡å‹é€‰æ‹©åŠŸèƒ½ï¼Œæ ¹æ®è¾“å…¥çš„æ•°æ®é›†å’Œä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é¢„è®­ç»ƒå›¾åƒåˆ†å‰²æ¨¡å‹ã€‚å®ƒå¯ä»¥å¤„ç†ä¸åŒçš„è¾“å…¥æ•°æ®ç±»å‹å’Œä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ç­‰ã€‚å…·ä½“è€Œè¨€ï¼ŒAutoModelForImageSegmentationä¼šè‡ªåŠ¨é€‰æ‹©å¹¶åŠ è½½æœ€é€‚åˆè¾“å…¥æ•°æ®å’Œä»»åŠ¡ç±»å‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå¯¹è¾“å…¥çš„å›¾åƒè¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸åŒå¤§å°çš„åˆ†å‰²å›¾åƒï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ éƒ½è¢«åˆ†é…åˆ°é¢„å®šä¹‰çš„æ ‡ç­¾ã€‚é€šè¿‡å¯¹é½é¢„è®­ç»ƒæ¨¡å‹å’Œç›®æ ‡ä»»åŠ¡ï¼Œè¿›è¡Œå¾®è°ƒä»¥å®Œæˆå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚å®ƒå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†æ‰‹åŠ¨é€‰æ‹©å’Œé…ç½®æ¨¡å‹çš„å·¥ä½œé‡ã€‚



### AutoModelForMaskedImageModeling

> AutoModelForMaskedImageModelingæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºæ‰§è¡Œæ©è†œå›¾åƒå»ºæ¨¡ä»»åŠ¡ã€‚æ©è†œå›¾åƒå»ºæ¨¡æ˜¯æŒ‡å°†è¾“å…¥çš„å›¾åƒä¸­çš„æŸäº›éƒ¨åˆ†ç”¨æ©è†œè¿›è¡Œè¦†ç›–ï¼Œç„¶åå°è¯•é¢„æµ‹è¢«è¦†ç›–çš„éƒ¨åˆ†ã€‚è¿™ä¸ªä»»åŠ¡å¯ä»¥ç”¨äºå›¾åƒä¿®å¤ã€ç”µå½±ç‰¹æ•ˆã€æ–‡æœ¬è¯†åˆ«ç­‰ã€‚
> AutoModelForMaskedImageModelingåŸºäºæœ€å…ˆè¿›çš„æ©è†œå›¾åƒå»ºæ¨¡æ¨¡å‹ï¼Œå¦‚Mask R-CNNï¼ŒDETR-RCNNç­‰ã€‚å®ƒè¿˜æ”¯æŒåœ¨å¤šç§é¢„è®­ç»ƒçš„transformersæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚Bertï¼ŒRobertaï¼ŒElectraï¼Œç­‰ç­‰ã€‚



### AutoModelForMaskedLM

> AutoModelForMaskedLMæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºæ‰§è¡Œæ©è†œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚æ©è†œè¯­è¨€å»ºæ¨¡æ˜¯æŒ‡å°†è¾“å…¥çš„å¥å­ä¸­çš„ä¸€äº›å•è¯ä½¿ç”¨æ©è†œè¿›è¡Œè¦†ç›–ï¼Œç„¶åå°è¯•é¢„æµ‹è¢«è¦†ç›–çš„å•è¯ã€‚è¿™ä¸ªä»»åŠ¡å¯ä»¥ç”¨äºè®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€è‡ªåŠ¨æ‘˜è¦ç­‰ã€‚
>
> å®ƒå¯ä»¥æ ¹æ®è¾“å…¥çš„æ•°æ®å’Œä»»åŠ¡è¦æ±‚è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„Masked Language Modeling (MLM) æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒæˆ–è€…è®­ç»ƒã€‚
>
> Masked Language Modeling ç”¨masking tokenæ¥mask sequenceä¸­çš„ä¸€äº›tokensï¼Œç„¶åè°ƒæ•´æ¨¡å‹ä½¿ä¹‹ç”¨åˆé€‚çš„tokenæ¥å¡«å……è¿™äº›maskã€‚è¿™è®©æ¨¡å‹èƒ½å¤Ÿattend right contextï¼ˆmaskå³è¾¹çš„tokenï¼‰å’Œleft contextï¼ˆmaskå·¦è¾¹çš„tokenï¼‰ã€‚è¿™æ ·çš„è®­ç»ƒè®¾ç½®ä¸ºéœ€è¦bi-directional contextçš„ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚SQuAD1ï¼‰æä¾›äº†å¼ºåŸºç¡€ã€‚
>
> AutoModelForMaskedLMä½¿ç”¨äº†Hugging Face Transformersåº“ä¸­çš„transformers.AutoModelForPreTrainingå’Œtransformers.AutoConfigç±»ã€‚è¿™äº›ç±»å¯ä»¥è‡ªåŠ¨é€‰æ‹©åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„Transformeræ¨¡å‹ï¼Œå¹¶ç”Ÿæˆä¸æ•°æ®å’Œä»»åŠ¡åŒ¹é…çš„é…ç½®æ–‡ä»¶ã€‚
>
> å…·ä½“æ¥è¯´ï¼ŒAutoModelForMaskedLMå¯ä»¥é€‰æ‹©ä»¥ä¸‹å¸¸è§çš„MLMæ¨¡å‹ï¼š
>
> BERT	RoBERTa	ALBERT	DistilBERT	ELECTRA ...
>
> è¿™äº›æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨åŸºäºTransformersçš„encoder-decoderæ¶æ„ã€‚å…¶ä¸­BERTæ˜¯æœ€æ—©æå‡ºçš„MLMæ¨¡å‹ï¼Œåæ¥å‡ºç°çš„RoBERTaå’ŒALBERTåœ¨BERTçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼ŒåŸºæœ¬ä¸Šéƒ½å…·æœ‰æ¯”BERTæ›´å¥½çš„æ€§èƒ½ã€‚DistilBERTæ˜¯ä¸€ä¸ªè½»é‡çº§çš„BERTæ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¤§å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ã€‚ELECTRAæ˜¯ä¸€ç§åŸºäºå¯¹æŠ—è®­ç»ƒçš„MLMæ¨¡å‹ï¼Œç›¸å¯¹äºä¼ ç»Ÿçš„MLMæ¨¡å‹ï¼Œå®ƒå¯ä»¥æ›´å¿«åœ°è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜è®­ç»ƒæ•ˆæœã€‚



### AutoModelForMultipleChoice

>è¯¥ç±»å¯æ ¹æ®æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹è‡ªåŠ¨æ„å»ºå¤šé€‰é¢˜ä»»åŠ¡æ¨¡å‹ï¼Œæ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬BERTã€RoBERTaã€DistilBERTç­‰ã€‚è¿™äº›æ¨¡å‹çš„çµæ„Ÿæ¥è‡ªäºè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å°†é—®é¢˜å’Œæ¯ä¸ªå€™é€‰ç­”æ¡ˆç»“åˆèµ·æ¥ï¼Œå°†å…¶ç¼–ç ä¸ºå‘é‡ã€‚ç„¶åï¼Œåœ¨è¿™äº›å‘é‡çš„åŸºç¡€ä¸Šæ‰§è¡ŒäºŒå…ƒåˆ†ç±»çš„ä»»åŠ¡ï¼Œä»¥ç¡®å®šæœ€ä½³ç­”æ¡ˆã€‚
>
>ä½¿ç”¨AutoModelForMultipleChoiceï¼Œç”¨æˆ·å¯ä»¥æ›´æ–¹ä¾¿åœ°è®­ç»ƒè‡ªå·±çš„å¤šé€‰é¢˜æ¨¡å‹ï¼Œè¿‡ç¨‹ä¸­ä¹Ÿçœå»äº†ä»é¢„è®­ç»ƒæ¨¡å‹æ‰‹åŠ¨æ„å»ºæ¨¡å‹çš„æ­¥éª¤ã€‚



### AutoModelForObjectDetection

> AutoModelForObjectDetectionæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¨¡å‹é€‰æ‹©å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®è¾“å…¥çš„æ•°æ®å’Œä»»åŠ¡è¦æ±‚è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒæˆ–è€…è®­ç»ƒã€‚
>
>AutoModelForObjectDetectionä½¿ç”¨äº†Hugging Face Transformersåº“ä¸­çš„transformers.AutoModelForPreTrainingå’Œtransformers.AutoConfigç±»ã€‚è¿™äº›ç±»å¯ä»¥è‡ªåŠ¨é€‰æ‹©åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„Transformeræ¨¡å‹ï¼Œå¹¶ç”Ÿæˆä¸æ•°æ®å’Œä»»åŠ¡åŒ¹é…çš„é…ç½®æ–‡ä»¶ã€‚
>
>å…·ä½“æ¥è¯´ï¼ŒAutoModelForObjectDetectionå¯ä»¥é€‰æ‹©ä»¥ä¸‹å¸¸è§çš„ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼š
>
>Faster R-CNN	Mask R-CNN	RetinaNet	YOLOv3	EfficientDet ...
>åŒæ—¶ï¼Œè¿˜å¯ä»¥é€‰æ‹©å„ç§åŸºäºTransformersçš„backboneï¼Œå¦‚BERTã€RoBERTaã€DistilBERTç­‰ã€‚



### AutoModelForQuestionAnswering

>AutoModelForQuestionAnsweringæ˜¯ä¸€ä¸ªåŸºäºtransformersé¢„è®­ç»ƒæ¨¡å‹çš„å·¥å…·ï¼Œå®ƒçš„ä½œç”¨æ˜¯ä¸ºç»™å®šçš„é—®ç­”ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚
>
>å…·ä½“æ¥è¯´ï¼ŒAutoModelForQuestionAnsweringå°†è¾“å…¥çš„é—®é¢˜ï¼ˆquestionï¼‰å’Œä¸€ä¸ªå¯èƒ½åŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬æ®µè½ï¼ˆcontextï¼‰è¿›è¡Œå¤„ç†ï¼Œç„¶åä½¿ç”¨å¦‚RoBERTaã€Bertç­‰transformersé¢„è®­ç»ƒæ¨¡å‹çš„ç»“æ„å¯¹é—®é¢˜å’Œæ–‡æœ¬æ®µè½è¿›è¡Œç¼–ç ï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä¸ªåŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬ç‰‡æ®µã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒAutoModelForQuestionAnsweringä½¿ç”¨äº†å¦‚softmaxç­‰æŸå¤±å‡½æ•°æ¥å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚
>
>AutoModelForQuestionAnsweringæ¶‰åŠçš„é¢„è®­ç»ƒæ¨¡å‹åŒ…æ‹¬BertForQuestionAnsweringã€RobertaForQuestionAnsweringå’ŒDistilBertForQuestionAnsweringç­‰ã€‚è¿™äº›æ¨¡å‹ç”±äºåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—çš„æ˜¾è‘—æˆæœï¼Œè¢«å¹¿æ³›åº”ç”¨äºé—®ç­”ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ã€æ‘˜è¦ç”Ÿæˆã€è¯­è¨€ç†è§£ç­‰å¤šä¸ªé¢†åŸŸã€‚
>
>AutoModelForQuestionAnsweringå¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´åŠ é«˜æ•ˆåœ°æ„å»ºé«˜æ€§èƒ½çš„é—®ç­”ç³»ç»Ÿï¼Œé™ä½æ¨¡å‹å¼€å‘å’Œä¼˜åŒ–çš„æˆæœ¬ã€‚



### AutoModelForSemanticSegmentation

> AutoModelForSemanticSegmentationæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºæ‰§è¡Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚å®ƒä½¿ç”¨äº†è‡ªåŠ¨åŒ–æ¨¡å‹é€‰æ‹©çš„æŠ€æœ¯ï¼ŒåŸºäºç»™å®šçš„ä»»åŠ¡ã€è¯­è¨€å’Œå…¶ä»–è¶…å‚æ•°è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥æ˜¯å›¾åƒï¼Œè¾“å‡ºæ˜¯æ¯ä¸ªåƒç´ çš„ç±»åˆ«æ ‡ç­¾ï¼Œå®ƒå¯ä»¥å°†åƒç´ åˆ†ä¸ºä¸åŒçš„åŒºåŸŸï¼Œä¾‹å¦‚â€œäººâ€ï¼Œâ€œè½¦â€ç­‰ã€‚æ—¨åœ¨å°†å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ ä¸å…¶å¯¹åº”çš„ç‰©ä½“æˆ–åœºæ™¯ç±»åˆ«å…³è”èµ·æ¥
>      
>AutoModelForSemanticSegmentationåŸºäºæœ€å…ˆè¿›çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå¦‚U-Netï¼ŒSegNetï¼ŒDeepLabç­‰ã€‚å®ƒè¿˜æ”¯æŒåœ¨å¤šç§é¢„è®­ç»ƒçš„transformersæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚Bertï¼ŒRobertaï¼ŒElectraï¼Œç­‰ç­‰ã€‚
>
>
>è¯¥ç±»å¯æ ¹æ®æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹è‡ªåŠ¨æ„å»ºè¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œæ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬UNetã€DeepLabV3ã€PSPNetç­‰ã€‚è¿™äº›æ¨¡å‹æ—¨åœ¨é€šè¿‡ç¼–ç å›¾åƒç‰¹å¾æ¥æ¨æ–­æ¯ä¸ªåƒç´ åŒ…å«çš„ç±»åˆ«æ ‡ç­¾ï¼Œå¹¶ä¸”åœ¨è®¸å¤šè§†è§‰ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†å‡ºè‰²çš„æ•ˆæœã€‚
>
>ä½¿ç”¨AutoModelForSemanticSegmentationï¼Œç”¨æˆ·å¯ä»¥æ›´æ–¹ä¾¿åœ°è®­ç»ƒè‡ªå·±çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œè¿‡ç¨‹ä¸­ä¹Ÿçœå»äº†ä»é¢„è®­ç»ƒæ¨¡å‹æ‰‹åŠ¨æ„å»ºæ¨¡å‹çš„æ­¥éª¤ã€‚



### AutoModelForSeq2SeqLM

> AutoModelForSeq2SeqLMæ˜¯ä¸€ä¸ªåœ¨ç»™å®šçš„æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œåºåˆ—åˆ°åºåˆ—ç”Ÿæˆä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©å·¥å…·ã€‚å®ƒå¯ä»¥è‡ªåŠ¨é€‰æ‹©é€‚åˆç»™å®šæ•°æ®é›†çš„æœ€ä½³é¢„è®­ç»ƒseq2seqæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ç‰¹å®šçš„åºåˆ—åˆ°åºåˆ—ç”Ÿæˆä»»åŠ¡ã€‚
>
>å…·ä½“æ¥è¯´ï¼Œå®ƒå°†è¾“å…¥çš„æ–‡æœ¬æ•°æ®è¿›è¡Œtokenizationï¼ˆæ ‡è®°åŒ–ï¼‰ï¼Œå¹¶ä½¿ç”¨encoder-decoderçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼ˆå¦‚T5ã€Bartã€Pegasusç­‰ï¼‰å¯¹è¾“å…¥åºåˆ—è¿›è¡Œç¼–ç ï¼Œç„¶åæ ¹æ®ç”Ÿæˆä»»åŠ¡çš„ç±»å‹ï¼ˆå¦‚ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ç­‰ï¼‰å¯¹è¾“å‡ºåºåˆ—è¿›è¡Œè§£ç ç”Ÿæˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒä½¿ç”¨äº†å¦‚beam searchç­‰æŠ€æœ¯å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨cross entropyç­‰æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚AutoModelForSeq2SeqLMå¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´å¿«åœ°æ‰¾åˆ°é€‚åˆè‡ªå·±åº”ç”¨åœºæ™¯çš„seq2seqæ¨¡å‹ï¼Œä»è€ŒåŠ é€Ÿæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ã€‚

### AutoModelForSequenceClassification

>    AutoModelForSequenceClassificationæ˜¯ä¸€ä¸ªåœ¨ç»™å®šçš„æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œåˆ†ç±»ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©çš„å·¥å…·ã€‚å®ƒå¯ä»¥è‡ªåŠ¨æ ¹æ®ç»™å®šçš„æ•°æ®é›†é€‰æ‹©æœ€é€‚åˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè‡ªåŠ¨æ„å»ºä¸€ä¸ªé€‚ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹,å¹¶å¯¹è¯¥æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šçš„åˆ†ç±»ä»»åŠ¡ã€‚
>    
>    å…·ä½“æ¥è¯´ï¼Œå®ƒå°†è¾“å…¥çš„æ–‡æœ¬æ•°æ®å¤„ç†ä¸ºtoken embeddingsï¼Œç„¶åå°†è¿™äº›embeddingsè¾“å…¥åˆ°BERTã€RoBERTaã€XLNetã€DistilBERTã€ALBERTç­‰å¸¸è§çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œç„¶åå°†æœ€åä¸€ä¸ªéšè—å±‚çš„è¡¨ç¤ºç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒä½¿ç”¨äº†äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚è¯¥æ¨¡å‹å¯ä»¥æ¥å—è¾“å…¥åºåˆ—å¹¶è¾“å‡ºè¯¥åºåˆ—å¯¹åº”çš„æ ‡ç­¾ã€‚
>    
>    ä½¿ç”¨AutoModelForSequenceClassificationï¼Œç”¨æˆ·å¯ä»¥æ›´å®¹æ˜“åœ°è®­ç»ƒè‡ªå·±çš„åºåˆ—åˆ†ç±»æ¨¡å‹ï¼Œçœå»äº†æ‰‹åŠ¨ä»é¢„è®­ç»ƒæ¨¡å‹æ„å»ºæ¨¡å‹çš„éº»çƒ¦ã€‚
>    
>    ä½¿ç”¨AutoModelForSequenceClassificationçš„ä¸€äº›å¸¸è§çš„é¢„è®­ç»ƒæ¨¡å‹åŒ…æ‹¬BERTã€RoBERTaã€XLNetå’ŒDistilBERTç­‰ã€‚

### AutoModelForSpeechSeq2Seq

>AutoModelForSpeechSeq2Seqæ˜¯ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒçš„è‡ªåŠ¨æ¨¡å‹ï¼Œç”¨äºå°†è¾“å…¥çš„è¯­éŸ³åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä½¿ç”¨åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰ç¥ç»ç½‘ç»œæ¥å­¦ä¹ å°†è¾“å…¥è¯­éŸ³åºåˆ—æ˜ å°„åˆ°ç›¸åº”çš„æ–‡æœ¬åºåˆ—ã€‚ç”¨äºè¿›è¡Œè¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ç­‰è¯­éŸ³åºåˆ—è½¬æ¢ä»»åŠ¡ã€‚
>
>éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒAutoModelForSpeechSeq2Seqéœ€è¦ä½¿ç”¨ç‰¹å®šçš„è¾“å…¥æ•°æ®æ ¼å¼ï¼Œå³éŸ³é¢‘æ–‡ä»¶å’Œç›¸åº”çš„æ ‡ç­¾æ–‡ä»¶ã€‚éŸ³é¢‘æ–‡ä»¶åº”è¯¥æ˜¯wavæ ¼å¼ï¼Œæ ‡ç­¾æ–‡ä»¶åº”è¯¥æ˜¯ä¸éŸ³é¢‘æ–‡ä»¶ç›¸åŒé•¿åº¦çš„æ–‡æœ¬åºåˆ—ï¼Œå¯ä»¥ä½¿ç”¨æ‹¼éŸ³æˆ–å­—ç¬¦è¡¨ç¤ºã€‚åŒæ—¶ï¼Œè¿˜éœ€è¦è¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œæ•°æ®å¢å¼ºæ“ä½œï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚
>
>ä½¿ç”¨AutoModelForSpeechSeq2Seqå¯ä»¥æå¤§åœ°ç®€åŒ–è¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å®ç°è¿‡ç¨‹ï¼Œä½¿å¾—å¼€å‘è€…æ— éœ€æ‰‹åŠ¨æ„å»ºå¤æ‚çš„æ¨¡å‹å’Œç‰¹å¾å·¥ç¨‹æµç¨‹ï¼Œåªéœ€å°†åŸå§‹è¯­éŸ³æ•°æ®è¾“å…¥æ¨¡å‹å³å¯è·å¾—å¯¹åº”çš„æ–‡æœ¬è¾“å‡ºã€‚
>
>AutoModelForSpeechSeq2Seqä½¿ç”¨çš„å…·ä½“æ¨¡å‹åŒ…æ‹¬ï¼šWav2Vec2ã€S2Tã€M2Mç­‰ã€‚å…¶ä¸­ï¼ŒWav2Vec2æ˜¯åŸºäºè‡ªç›‘ç£çš„è¯­éŸ³ç‰¹å¾æå–æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„æ€§èƒ½ã€‚S2Tæ˜¯ä¸€ç§å°†è¯­éŸ³åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—çš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯­éŸ³è½¬å†™çš„ä»»åŠ¡ã€‚M2Måˆ™æ˜¯ä¸€ç§å°†è¯­éŸ³åºåˆ—è½¬æ¢ä¸ºè¯­éŸ³åºåˆ—ï¼ˆç¿»è¯‘ï¼‰çš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè·¨è¯­è¨€çš„è¯­éŸ³ç¿»è¯‘ä»»åŠ¡ã€‚

### AutoModelForTokenClassification

>AutoModelForTokenClassificationæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŸºäºé¢„è®­ç»ƒçš„è‡ªåŠ¨æ¨¡å‹ï¼Œç”¨äºå°†è¾“å…¥çš„æ–‡æœ¬åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°åˆ†ç±»ä¸ºé¢„å®šä¹‰çš„æ ‡è®°ç±»å‹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°ä¸¢ç»™æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œä»¥ç¡®å®šå…¶å±äºå“ªä¸ªæ ‡è®°ç±»å‹ã€‚
>
> å¯ä»¥è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognitionï¼‰ä»»åŠ¡ã€‚ç”¨äºè¿›è¡Œå‘½åå®ä½“è¯†åˆ«å’Œåºåˆ—æ ‡æ³¨ä»»åŠ¡ã€‚å®ƒèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«è¾“å…¥å¥å­ä¸­çš„è¯æ±‡ï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­å…¶å±äºå“ªç§ç±»å‹çš„å®ä½“ï¼ˆå¦‚äººåã€ç»„ç»‡æœºæ„ã€æ—¶é—´ç­‰ï¼‰ã€‚å®ƒæ˜¯åŸºäºtransformersé¢„è®­ç»ƒçš„æ¨¡å‹å’ŒPyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶æ„å»ºçš„ï¼Œæ ¹æ®ä»»åŠ¡éœ€è¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æé«˜åœ¨å…·ä½“ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
>
>  AutoModelForTokenClassificationä½¿ç”¨äº†Hugging Face Transformersåº“ä¸­çš„transformers.AutoModelForTokenClassificationå’Œtransformers.AutoConfigç±»ã€‚è¿™äº›ç±»å¯ä»¥è‡ªåŠ¨é€‰æ‹©åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„Transformeræ¨¡å‹ï¼Œå¹¶ç”Ÿæˆä¸æ•°æ®å’Œä»»åŠ¡åŒ¹é…çš„é…ç½®æ–‡ä»¶ã€‚
>
>  å…·ä½“æ¥è¯´ï¼ŒAutoModelForTokenClassificationå¯ä»¥é€‰æ‹©ä»¥ä¸‹å¸¸è§çš„Token Classificationæ¨¡å‹ï¼š
>
>â€‹        BERT ï¼šæ˜¯è°·æ­Œå¼€å‘çš„ä¸€ç§åŒå‘è½¬æ¢ç¼–ç å™¨ï¼Œåœ¨å„ç§NLPä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚
>  RoBERTa ï¼šæ˜¯ç”±Facebookå›¢é˜Ÿä¼˜åŒ–çš„BERTæ¨¡å‹ï¼Œæé«˜äº†åœ¨ä¸€äº›NLPä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
>â€‹        XLNet ï¼šæ˜¯ç”±CMUå’ŒGoogleå¼€å‘çš„ä¸€ç§è‡ªè®¸å¤šNLPä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚
> DistilBERT ï¼šæ˜¯ç”±Hugging Faceå¼€å‘çš„ä¸€ç§è½»é‡çº§çš„BERTå˜ä½“ï¼Œå…·æœ‰è¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´å’Œè¾ƒå°çš„æ¨¡å‹å¤§å°ï¼Œä½†					   åœ¨å¤šNLPä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚
>â€‹     ALBERT ï¼šç”±Googleå¼€å‘çš„ä¸€ç§è½»é‡çº§BERTå˜ä½“ï¼Œé‡‡ç”¨äº†ä¸€ç§å‚æ•°å…±äº«ç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¸€äº›NLPå¥½çš„æ€§èƒ½ã€‚
>   ELECTRA ï¼šæ˜¯ä¸€ç§æ–°å‹çš„é¢„è®­ç»ƒæ€è·¯ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ã€ç”Ÿæˆã€QAç­‰ä»»åŠ¡ã€‚
>
>è¿™äº›æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨åŸºäºTransformersçš„encoder-decoderæ¶æ„ï¼Œå…¶ä¸­BERTæ˜¯æœ€æ—©æå‡ºçš„æ¨¡å‹ï¼Œåæ¥å‡ºç°çš„RoBERTaå’ŒALBERTåœ¨BERTçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼ŒåŸºæœ¬ä¸Šéƒ½å…·æœ‰æ¯”BERTæ›´å¥½çš„æ€§èƒ½ã€‚
>XLNetæ˜¯ä¸€ç§æ–°å‹çš„è‡ªå›å½’æ¨¡å‹ï¼Œå¯ä»¥åœ¨åºåˆ—æ ‡æ³¨ä»»åŠ¡ä¸­æä¾›æ›´å¥½çš„æ€§èƒ½ã€‚XLMæ—¨åœ¨è§£å†³è·¨è¯­ç§è‡ªç„¶è¯­è¨€ç†è§£é—®é¢˜ï¼Œå¯ä»¥åœ¨å¤šè¯­ç§åºåˆ—æ ‡æ³¨ä»»åŠ¡ä¸­æä¾›æ›´å¥½çš„æ€§èƒ½ã€‚
>DistilBERTæ˜¯ä¸€ä¸ªè½»é‡çº§ç‰ˆæœ¬çš„BERTæ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¤§å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ï¼Œé€šè¿‡å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—è´Ÿè½½æ¥æé«˜æ•ˆç‡å’Œé€Ÿåº¦ã€‚
>ELECTRAä¹Ÿæ˜¯ä¸€ç§åŸºäºå¯¹æŠ—è®­ç»ƒçš„æ¨¡å‹ï¼Œç›¸å¯¹äºä¼ ç»Ÿçš„Token Classificationæ¨¡å‹ï¼Œå®ƒå¯ä»¥æ›´å¿«åœ°è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜è®­ç»ƒæ•ˆæœã€‚è¿™äº›æ¨¡å‹éƒ½å¯ä»¥ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognitionï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPart-of-Speech Taggingï¼‰ã€è¯­ä¹‰è§’è‰²æ ‡æ³¨ï¼ˆSemantic Role Labelingï¼‰ç­‰ä»»åŠ¡ã€‚

### AutoModelForVision2Seq

> AutoModelForVision2Seqæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¸»è¦ç”¨äºå›¾åƒè¯†åˆ«å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ä»»åŠ¡ã€‚å®ƒèƒ½å¤Ÿå°†è¾“å…¥çš„å›¾åƒè½¬åŒ–ä¸ºå¯¹åº”çš„æ–‡å­—æè¿°è¾“å‡ºã€‚
>
>è¯¥æ¨¡å‹é€šå¸¸åŸºäºtransformersé¢„è®­ç»ƒæ¨¡å‹å’ŒPyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶æ„å»ºï¼Œå¯ä»¥é€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–æ•°æ®é›†ã€‚å®ƒèƒ½å¤Ÿå­¦ä¹ åˆ°å›¾åƒä¸å¯¹åº”è‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¹¶å¯¹æœªçŸ¥çš„å›¾åƒè¿›è¡Œè‡ªåŠ¨æè¿°ï¼Œä»è€Œå®ç°å›¾åƒè‡ªåŠ¨ç”Ÿæˆæ–‡å­—æè¿°çš„åŠŸèƒ½ã€‚
>
>åœ¨ä¸€äº›åº”ç”¨åœºæ™¯ä¸­ï¼Œä¾‹å¦‚å›¾ç‰‡æœç´¢ã€è‡ªåŠ¨å›¾åƒæè¿°ç”Ÿæˆã€å›¾åƒç‰‡æ®µç»„åˆç­‰ï¼Œåœ¨å›¾åƒç†è§£ä¸è‡ªç„¶è¯­è¨€å¤„ç†ä¸¤ä¸ªé¢†åŸŸçš„äº¤å‰ç‚¹ä¸Šï¼ŒAutoModelForVision2Seqæœ‰ç€å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚ 

------

## ä¸¾ä¸€äº›ä¾‹å­

**AutoModelForSequenceClassification**

 Sequence Classificationä»»åŠ¡æ˜¯å°†sequenceåœ¨ç»™å®šçš„ç±»æ•°ä¸­è¿›è¡Œåˆ†ç±»ã€‚å¦‚GLUEæ•°æ®é›†ã€‚

ç”¨AutoClassåˆ¤æ–­ä¸¤å¥è¯æ˜¯å¦åŒä¹‰ï¼ˆäº’ä¸ºæ”¹å†™ï¼‰çš„ç¤ºä¾‹ï¼š

- æ ¹æ®checkpointååˆå§‹åŒ–tokenizerå’Œæ¨¡å‹ï¼Œæ¨¡å‹æ¶æ„æ˜¯BERTï¼Œå¹¶åŠ è½½checkpointä¸­çš„æƒé‡
- æ„å»ºä¸€ä¸ªç”±ä¸¤å¥è¯ç»„æˆçš„sequenceï¼Œå«æœ‰æ­£ç¡®çš„model-specific separators, token type ids and attention masksï¼ˆç”±tokenizerè‡ªåŠ¨ç”Ÿæˆï¼‰
- å°†è¿™ä¸ªsequenceä¼ å…¥æ¨¡å‹ï¼Œå¯¹å®ƒè¿›è¡Œåˆ†ç±»ï¼šæ˜¯å¦åŒä¹‰
- è®¡ç®—è¾“å‡ºçš„softmaxç»“æœï¼Œè·å¾—åœ¨å„ç±»ä¸Šçš„æ¦‚ç‡å€¼
- æ‰“å°ç»“æœ

```
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")

paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits

paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
```

è¾“å‡ºï¼š

```
`not paraphrase: 10%`
`is paraphrase: 90%`
not paraphrase: 94%`
`is paraphrase: 6%
```

------

### **AutoModelForQuestionAnswering**

ä»contextï¼ˆä¸€æ®µæ–‡æœ¬ï¼‰ä¸­æŠ½å–å¥å­ï¼Œä½œä¸ºç‰¹å®šé—®é¢˜ç­”å¥ã€‚å¦‚SQuAD[1](https://blog.csdn.net/PolarisRisingWar/article/details/123575883#fn1)æ•°æ®é›†ã€‚

ç”¨AutoClassçš„ç¤ºä¾‹ï¼š

- æ ¹æ®checkpointååˆå§‹åŒ–tokenizerå’Œæ¨¡å‹ï¼Œæ¨¡å‹æ¶æ„æ˜¯BERTï¼Œå¹¶åŠ è½½checkpointä¸­çš„æƒé‡
- å®šä¹‰contextå’Œä¸€äº›é—®é¢˜
- è¿­ä»£æ‰€æœ‰é—®é¢˜ï¼Œæ„å»ºcontextå’Œå½“å‰é—®é¢˜çš„sequenceï¼ˆç”¨æ­£ç¡®çš„model-specific separators, token type ids and attention masksï¼‰
- å°†è¿™ä¸ªsequenceä¼ å…¥æ¨¡å‹ï¼Œè¾“å‡ºæ•´ä¸ªsequenceä¸Šæ¯ä¸ªtokençš„å¾—åˆ†ï¼ˆè¯¥tokenæ˜¯start indexæˆ–end indexçš„å¯èƒ½æ€§å¾—åˆ†ï¼‰ã€‚
- è®¡ç®—è¾“å‡ºçš„softmaxç»“æœï¼Œè·å¾—åœ¨å„tokenä¸Šçš„æ¦‚ç‡å€¼
- è·å–è¢«è¯†åˆ«ä¸ºstartå’Œendä¹‹é—´çš„å€¼çš„tokenï¼Œå°†å…¶è½¬åŒ–ä¸ºå­—ç¬¦ä¸²
- æ‰“å°ç»“æœ

```
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

text = r"""
ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
"""
questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]

for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]

    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits

    # Get the most likely beginning of answer with the argmax of the score
    answer_start = torch.argmax(answer_start_scores)
    # Get the most likely end of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1

    answer = tokenizer.convert_tokens_to_string(
        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
    )

    print(f"Question: {question}")
    print(f"Answer: {answer}")
```

è¾“å‡ºï¼š

```
Question: How many pretrained models are available in ğŸ¤— Transformers?
Answer: over 32 +
Question: What does ğŸ¤— Transformers provide?
Answer: general - purpose architectures
Question: ğŸ¤— Transformers provides interoperability between which frameworks?
Answer: tensorflow 2. 0 and pytorch
```

### Language Modeling

Language modelingæ˜¯ä½¿æ¨¡å‹é€‚åº”æŸä¸€è¯­æ–™ï¼ˆä¸€èˆ¬æ˜¯ç‰¹å®šé¢†åŸŸçš„ï¼‰çš„ä»»åŠ¡

###  AutoModelForMaskedLM

Masked Language Modeling ç”¨masking tokenæ¥mask sequenceä¸­çš„ä¸€äº›tokensï¼Œç„¶åè°ƒæ•´æ¨¡å‹ä½¿ä¹‹ç”¨åˆé€‚çš„tokenæ¥å¡«å……è¿™äº›maskã€‚è¿™è®©æ¨¡å‹èƒ½å¤Ÿattend right contextï¼ˆmaskå³è¾¹çš„tokenï¼‰å’Œleft contextï¼ˆmaskå·¦è¾¹çš„tokenï¼‰ã€‚è¿™æ ·çš„è®­ç»ƒè®¾ç½®ä¸ºéœ€è¦bi-directional contextçš„ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚SQuAD1ï¼‰æä¾›äº†å¼ºåŸºç¡€ã€‚

ç”¨AutoClassçš„ç¤ºä¾‹ï¼š

- æ ¹æ®checkpointååˆå§‹åŒ–tokenizerå’Œæ¨¡å‹ï¼Œæ¨¡å‹æ¶æ„æ˜¯DistilBERTï¼Œå¹¶åŠ è½½checkpointä¸­çš„æƒé‡
- å®šä¹‰å«æœ‰ä¸€ä¸ªmasked tokençš„sequenceï¼šç”¨tokenizer.mask_tokenï¼ˆè¿™æ˜¯ä¸ªå­—ç¬¦ä¸²æ ¼å¼çš„å˜é‡ï¼Œåœ¨å­—ç¬¦ä¸²ä¸­ç”¨èŠ±æ‹¬å·æ‹¬èµ·æ¥ä»¥å®ç°æ›¿æ¢2ï¼‰æ›¿æ¢ä¸€ä¸ªå•è¯ï¼ˆæˆ‘æ„Ÿè§‰è¿™é‡Œçš„å•è¯åº”è¯¥æŒ‡çš„æ˜¯ä¸€ä¸ªtokenï¼‰
- å°†sequenceç¼–ç ä¸ºtoken IDsçš„åˆ—è¡¨ï¼Œæ‰¾åˆ°masked tokenåœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ã€‚
- æå–åœ¨mask tokenç´¢å¼•å€¼å¤„çš„é¢„æµ‹å€¼ï¼šè¿™ä¸ªå¼ é‡å’Œvocabularyæœ‰åŒæ ·çš„å°ºå¯¸ï¼Œå…¶å…ƒç´ å€¼å°±æ˜¯åˆ†é…ç»™æ¯ä¸ªtokençš„å¾—åˆ†ã€‚æ¨¡å‹è®¤ä¸ºåœ¨ç»™å®šcontextä¸‹ï¼Œæ›´æœ‰å¯èƒ½æ˜¯è¿™ä¸ªmasked tokençš„tokenï¼Œä¼šå¾—åˆ°æ›´é«˜çš„åˆ†æ•°ã€‚
- ç”¨PyTorchçš„topkæ–¹æ³•æå–å¾—åˆ†æœ€é«˜çš„5ä¸ªtokenã€‚
- ç”¨ä¸Šè¿°çš„tokensæ¥æ›¿ä»£mask tokenï¼Œæ‰“å°ç»“æœã€‚

```
from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-cased")

sequence = (
    "Distilled models are smaller than the models they mimic. Using them instead of the large "
    f"versions would help {tokenizer.mask_token} our carbon footprint."
)

inputs = tokenizer(sequence, return_tensors="pt")
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]

token_logits = model(**inputs).logits
mask_token_logits = token_logits[0, mask_token_index, :]

top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
#å¾—åˆ°åˆ†æ•°æœ€é«˜çš„5ä¸ªtokençš„ç´¢å¼•
#å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œtopkå‡½æ•°é»˜è®¤æ˜¯æ ¹æ®valueç»è¿‡sortçš„ã€‚å‚è€ƒå…¶å‡½æ•°æ–‡æ¡£ï¼šhttps://pytorch.org/docs/stable/generated/torch.topk.html

for token in top_5_tokens:
    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))
    #å°†è¯¥tokenè§£ç ä¸ºæ–‡æœ¬å½¢å¼ï¼Œæ›¿ä»£åŸæ–‡ä¸­çš„tokenizer.mask_token
```

è¾“å‡ºï¼š

```
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.
```

###  **AutoModelForCausalLM**

 CLMæ˜¯é¢„æµ‹ä¸€ä¸ªsequenceä¹‹åçš„tokençš„ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å¢ƒä¸‹ï¼Œæ¨¡å‹åªä¼šattend left contextï¼ˆmaskå·¦è¾¹çš„tokenï¼‰ã€‚è¿™æ ·çš„è®­ç»ƒè®¾ç½®ç‰¹åˆ«å…³æ³¨äºç”Ÿæˆä»»åŠ¡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ˜¯é€šè¿‡æŠ½æ ·è¾“å…¥sequenceå¾—åˆ°çš„æœ€åä¸€å±‚hidden stateçš„logitså¾—åˆ°çš„ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ˜¯é€šè¿‡æŠ½æ ·è¾“å…¥sequenceå¾—åˆ°çš„æœ€åä¸€å±‚hidden stateçš„logitså¾—åˆ°çš„ã€‚

ç”¨AutoClassçš„ç¤ºä¾‹ï¼šç”¨AutoModelForCausalLMã€AutoTokenizerå’Œtop_k_top_p_filtering()æ–¹æ³•ï¼Œåœ¨è¾“å…¥sequenceåæŠ½æ ·å¾—åˆ°ä¸‹ä¸€ä¸ªtokenï¼š

```
from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering
import torch
from torch import nn

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

sequence = f"Hugging Face is based in DUMBO, New York City, and"

inputs = tokenizer(sequence, return_tensors="pt")
input_ids = inputs["input_ids"]

# get logits of last hidden state
next_token_logits = model(**inputs).logits[:, -1, :]

# filter
filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)

# sample
probs = nn.functional.softmax(filtered_next_token_logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)

generated = torch.cat([input_ids, next_token], dim=-1)

resulting_string = tokenizer.decode(generated.tolist()[0])
print(resulting_string)
```

è¾“å‡ºï¼š

```
`Hugging Face is based in DUMBO, New York City, and is`
```

------

ä»¥ä¸Šä¸»è¦æ˜¯å¯¹Transformer å­¦ä¹ çš„ç›¸å…³çš„åŸºæœ¬çŸ¥è¯†åšä¸ªæ•´ç†æ€»ç»“ï¼Œå¸Œæœ›èƒ½å¸®åŠ©åˆ°å¤§å®¶ã€‚

---

## åœ¨çº¿demo

> -  [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+[MASK]+of+France)
> - [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)

## ç›¸å…³é˜…è¯»

> - [Transformersä¸­æ–‡ä½¿ç”¨è¯´æ˜](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md)
>- [æ•°æ®é›†æŸ¥æ‰¾](https://huggingface.co/datasets)
> - [arXiv:1810.04805](https://arxiv.org/abs/1810.04805), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
